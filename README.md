# Facial Emotion Recognition over 7 spectrum of Emotions using Convolutional Neural Networks. 
## Executive Summary
Our project delves into the intricate world of real-time facial emotion recognition, leveraging technologies like Convolutional Neural Networks (CNNs), Residual Networks (ResNets), VGG-16, and the influential FER2013 dataset. By employing these advanced deep learning models, we tend to look upon a zone where machines comprehend human emotions through facial expressions with unparalleled accuracy and efficiency.  
We began with a comprehensive exploration of facial emotion recognition, tracing its evolution from traditional methods to the transformative impact of CNNs and ResNets. Through meticulous analysis of the FER2013 dataset, we identified seven fundamental human emotions, mapping them to distinctive facial expressions. The integration of CNNs, particularly ResNet and VGG-16 architectures, empowered us to decode these expressions in real-time, opening avenues for applications in diverse sectors.  
In the realm of business, our project envisions a future where customer experiences are finely tailored. Marketing strategies are optimized based on customers’ emotional responses, fostering engagement and loyalty. Retail spaces dynamically adjust layouts in response to shoppers’ emotions, creating immersive and personalized environments. Furthermore, mental health services are revolutionized, enabling therapists to provide timely and tailored interventions. Educational platforms become adaptive, enhancing learning experiences through personalized guidance.  
In essence, our project stands at the intersection of cutting-edge technology, human emotions etc. It heralds a future where machines empathetically understand and respond to human emotions, fostering deeper connections and enhancing various aspects of society and business. Through this innovative fusion of technology and empathy, we pave the way for a more empathetic, efficient, and interconnected world.  

## Introduction
### **Problem Statement**  

**Facial Emotion Recognition over 7 spectrum of Emotions using Convolutional Neural Networks.**  
In recent years, advancements in artificial intelligence and machine learning have revolutionized various fields, including computer vision. One of the groundbreaking applications of these technologies is Real-Time Facial Emotion Recognition, a field that has gained significant attention due to its wide range of practical applications in areas like human-computer interaction, marketing research, and mental health diagnostics. At the heart of this innovation lies Convolutional Neural Networks (CNNs), a class of deep learning algorithms specifically designed for image analysis and recognition tasks.

**History of CNN and Its Real-Life Applications**  
Convolutional Neural Networks (CNNs) have their roots in the 1950s when neuroscientists and computer scientists began exploring the concept of artificial neural networks. However, it wasn't until the 1990s and 2000s that significant progress was made, leading to the development of modern CNN architecture. LeNet-5, introduced by Yann Lecun and his colleagues in 1998, marked a significant milestone, demonstrating the effectiveness of CNNs in handwritten digit recognition.Over the years, CNNs have found diverse real-life applications, ranging from image and speech recognition to autonomous vehicles and medical image analysis. In the context of facial emotion recognition, CNNs have enabled the creation of sophisticated systems capable of analyzing facial expressions in real time, leading to improved human-computer interactions and emotional analysis.

**Facial Recognition Systems Around the World**  
Facial recognition technology has become increasingly prevalent in various countries and industries. Governments and law enforcement agencies in several countries have deployed facial recognition systems for surveillance, security, and criminal identification purposes. Additionally, private companies have adopted facial recognition for authentication and personalized customer experiences. For instance, in China, facial recognition is widely used for payments, access control, and even in smart classrooms. In the United States, facial recognition technology is employed at airports and border crossings to enhance security measures.

**Emotions and CNN**  
In the widespread genre of human communication, emotions play a fundamental role, serving as the silent language that conveys our innermost feelings. Understanding these emotions is not just a facet of human psychology but has also become a focal point in the realm of artificial intelligence and computer vision. At the heart of this intersection lies the revolutionary technology of Convolutional Neural Networks (CNNs), designed to decipher the intricate nuances of human expression, paving the way for real-time facial emotion recognition. For emotion recognition, facial recognition systems have been integrated with CNNs to create emotionally intelligent systems. These systems can detect and analyze facial expressions in real time, enabling applications in various fields such as customer service, mental health diagnostics, and entertainment.  
Emotions, the intricate dance of **joy, sadness, anger, surprise, fear, and disgust,** are the essence of human experience. Rooted deeply in psychology, these emotions are not mere fleeting feelings but are the foundation upon which human interactions are built. Our faces, the most expressive canvas, vividly mirror these emotions, creating a visual language that transcends barriers. The study of emotions, known as affective computing, delves into this profound realm, seeking to understand the subtleties of human expression and translate them into the digital domain.  
Through Convolutional Neural Networks (CNNs), the technological marvels inspired by the human brain's intricate neural networks. These algorithms possess an unparalleled ability to dissect and interpret visual data, making them ideal instruments for deciphering facial expressions. By meticulously analyzing facial features, CNNs can discern the tiniest of muscle movements, capturing the essence of a smile or the furrow of a brow. Through layers of computations, CNNs transform pixels into emotions, creating a bridge between human expression and artificial intelligence.

**Emotional identification through facial features**  
The renowned philosopher and psychologist Paul Ekman, known for his pioneering work on emotions and facial expressions, once said, "Facial expressions are the universal language of emotion." This quote emphasizes the universal nature of facial expressions, transcending cultural and linguistic barriers, and underscores their significance in understanding human emotions.

For our project, we have considered the 7 basic emotions visible on any person’s face A brief summary about these emotions and the facial features through which these emotions can be detected has been discussed below as:-

- **Anger:** Anger is typically characterized by intense displeasure or rage. The facial expression includes furrowed brows, narrowed eyes, a tightened jaw, and lips pressed firmly together. These features signify a sense of aggression and hostility.  
- **Fear:** Fear arises in response to perceived threats or dangers. Facial expressions of fear include raised eyebrows, widened eyes, an open mouth, and tense facial muscles. This expression conveys a sense of alertness and readiness for action.  
- **Happiness:** Happiness reflects joy and contentment. It is displayed through smiling, twinkling eyes, raised cheeks, and sometimes squinting. The overall facial expression is relaxed and radiates positivity.  
- **Sadness:** Sadness is associated with feelings of sorrow or unhappiness. Downturned mouth, drooping eyelids, and a furrowed brow are typical facial cues of sadness. These expressions signify a sense of melancholy and emotional pain.  
- **Disgust:** Disgust conveys strong disapproval or revulsion. Facial expressions of disgust include nose wrinkling, a raised upper lip, narrowed eyes, and a lowered chin. These cues indicate a sense of aversion and distaste.  
- **Surprise:** Surprise is characterized by sudden astonishment or amazement. Widened eyes, raised eyebrows, a dropped jaw, and often accompanied by gasping are typical expressions of surprise. This emotion signifies a reaction to unexpected events or stimuli.
- **Neutral:** The neutral expression represents a lack of identifiable emotion. It is characterized by relaxed facial muscles and a neutral facial expression without specific emotional cues. The absence of specific expressions indicates a neutral emotional state.


## Methodology  
**About the Dataset**  
The FER2013 dataset is a widely used benchmark dataset in the field of facial expression recognition. FER2013 stands for "Facial Expression Recognition 2013." It was created by Pierre-Luc Carrier and Aaron Courville, two researchers from the University of Montreal. The dataset was designed to facilitate research and development in the area of facial expression recognition using machine learning algorithms, particularly deep learning models like Convolutional Neural Networks (CNNs).

The FER2013 dataset was compiled using images from various sources, including the internet and publicly available databases. These images were carefully curated to represent the seven basic human emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. Each image was labeled with the corresponding emotion category, enabling supervised learning techniques for emotion recognition tasks. FER2013 consists of facial images categorized into seven different emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. These emotions represent the basic human emotions widely 
studied in psychology. The images in the FER2013 dataset are relatively low-resolution grayscale images, with each image being 48x48 pixels in size. The dataset contains a total of 35,887 images, divided into three sets: 28,709 images for training, 3,589 images for validation, and 3,589 images for testing. The dataset's popularity is due in part to its large size and the variety of emotions represented, making it suitable for training deep learning algorithms. Researchers often preprocess and augment the FER2013 dataset to enhance its diversity and improve the performance of facial expression recognition models. This dataset has been instrumental in advancing the field of computer vision, enabling the development of more accurate and robust facial emotion recognition systems. While FER2013 is a valuable resource, it's worth noting that the images in the dataset often depict extreme or exaggerated facial expressions. Real-world scenarios might involve subtler expressions, which can pose a challenge when translating models trained on FER2013 to practical applications. One challenge in the FER2013 dataset is the class imbalance. Some emotions, such as happiness and neutral, are overrepresented in the dataset, while others, like disgust and fear, have fewer samples. Addressing this imbalance is essential for training models that can accurately recognize all emotions, as imbalanced datasets can lead to biased predictions. Researchers and practitioners widely use the FER2013 dataset for various purposes, including benchmarking new algorithms, comparing the performance of different models, and conducting experiments to improve facial expression recognition accuracy. The dataset has spurred numerous research papers and projects, contributing significantly to the evolution of emotion recognition technology. Deep learning models, particularly Convolutional Neural Networks (CNNs), are often trained on the FER2013 dataset. Researchers use techniques like data augmentation to artificially increase the dataset's size and diversity, helping the models generalize better to real-world scenarios 
with varied lighting conditions, facial orientations, and backgrounds. While FER2013 has been invaluable for research, it has certain limitations. The dataset primarily focuses on posed facial expressions, which might differ from spontaneous, real-life emotions. Additionally, the low resolution of images and the absence of contextual information can limit the models' performance, especially in complex, natural environments.

Despite these challenges, the FER2013 dataset remains a cornerstone in the field of facial expression recognition. It continues to inspire researchers to develop innovative techniques and models for understanding and interpreting human emotions through facial expressions, with applications ranging from human-computer interaction to mental health diagnostics.

![image](https://github.com/user-attachments/assets/aafa58e3-5ced-48bb-8145-3e626f603ef0)

### Techniques Used
* **AlexNet**  
  Utilizing AlexNet for facial expression recognition is a powerful choice, especially given its historical significance as one of the pioneering deep convolutional neural networks. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won the ImageNet Large Scale Visual Recognition Challenge in 2012, marking a significant breakthrough in the field of computer vision. Some of the key points to consider when using AlexNet for your project on real-time facial emotion recognition:  
   **Architecture:** AlexNet consists of five convolutional layers followed by three fully connectedlayers. The network employs the rectified linear unit (ReLU) activation function, which introduces non-linearity into the model. It also uses techniques such as dropout to prevent overfitting, and local response normalization for normalization between layers.
  **Pretrained Models:** Due to its success, pretrained versions of AlexNet are readily available in popular deep learning frameworks like TensorFlow and PyTorch. You can leverage these pretrained models and fine-tune them specifically for facial expression recognition using your dataset. Transfer learning, where you use a pretrained model and adapt it to your specific task, often leads to faster convergence and better performance.  
  **Data Preparation:** Ensure that your facial expression dataset is appropriately preprocessed to match the input requirements of AlexNet. This usually involves resizing images to the required input size (usually 224x224 pixels for AlexNet), normalizing pixel values, and structuring the data into appropriate training, validation, and test sets.  
  **Training Strategy:** Experiment with various hyperparameters during training, such as learning rates and batch sizes. Monitoring metrics like accuracy, loss, and validation performance can guide you in optimizing the training process. Additionally, consider employing techniques like learning rate schedules or early stopping to enhance training efficiency.  
  **Data Augmentation:** To increase the diversity of your training dataset and improve the model's robustness, apply data augmentation techniques. These can include random rotations, flips, and translations, simulating different facial orientations and lighting conditions.  
  **Evaluation and Fine-Tuning:** After training, evaluate the model's performance on your test dataset. If necessary, fine-tune the model by adjusting hyperparameters or employing techniques like regularization to achieve the desired accuracy and generalization to unseen data.

![image](https://github.com/user-attachments/assets/8cf33d14-6845-4b75-83e3-9a309edcc179)


* **VGG 16**  
  Certainly! Using VGG16, a popular deep learning architecture, for your facial emotion recognition project is a great choice. VGG16 is known for its simplicity and effectiveness, making it widely adopted in computer vision tasks. Here's how you can utilize VGG16 for your project:  
  **Architecture:** VGG16 is a deep convolutional neural network with 16 layers, including 13 convolutional layers and 3 fully connected layers. The network's architecture is characterized by its use of small 3x3 convolutional filters, which enables the model to learn intricate patterns in the data. VGG16's architecture follows a sequential pattern of convolutional layers, interspersed with max-pooling layers for spatial down-sampling.    
  **Transfer Learning:** One of the key advantages of VGG16 is its suitability for transfer learning. You can utilize pre-trained weights from the ImageNet dataset, a large dataset with millions of images across various categories. By leveraging these pre-trained weights, your model can benefit from the general features learned by VGG16 while fine-tuning the last few layers specifically for facial emotion recognition.  
  **Data Preprocessing:** Before feeding your data into the VGG16 model, it's crucial to 
preprocess your facial expression images. This typically involves resizing the images to the input size expected by VGG16 (usually 224x224 pixels), normalizing pixel values to the range [0, 1], and possibly augmenting the data to increase its diversity. Data augmentation techniques, such as rotation, flipping, and slight changes in brightness and contrast, can enhance the model's ability to generalize to different facial expressions.  
  **Model Modification:** Since you're dealing with facial emotion recognition, you'll need to modify the last few layers of VGG16 to suit your specific task. Replace the original output layer with a new softmax layer having seven units (one for each emotion category: anger, disgust, fear, happiness, sadness, surprise, and neutral). During training, the model will learn to map facial features to these emotion categories.  
  **Training and Evaluation:** Train your modified VGG16 model on your preprocessed FER2013 dataset. During training, monitor key metrics such as accuracy, loss, and validation performance to ensure your model is learning effectively. It's essential to split your dataset into training, validation, and test sets to properly evaluate the model's performance. You can fine-tune hyperparameters like learning rate and batch size based on the validation results.  
  **Fine-Tuning and Optimization:** After the initial training, you can experiment with fine-tuning different layers of the VGG16 model, especially if you find that the model's performance on your specific task could be further improved. Additionally, you can explore techniques such as learning rate schedules, early stopping, and regularization methods to optimize your model's performance and prevent overfitting.

![image](https://github.com/user-attachments/assets/ab10b362-3e13-4979-842e-adce1b05f4ec)

* **ResNet**  
  ResNet, short for Residual Neural Network, is a powerful and innovative deep learning architecture that addresses the vanishing gradient problem during the training of very deep neural networks. ResNet, introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in 2015, introduced a novel concept: residual learning. Traditional deep networks suffer from the vanishing gradient problem, where gradients diminish as they are back-propagated through the layers, making it challenging to train very deep networks. ResNet addresses this by utilizing residual blocks.  
  **Residual Blocks:** In ResNet, a residual block contains a shortcut connection, allowing the input to bypass one or more layers and directly feed into deeper layers. Mathematically, if \(x\) is the input to a set of layers, the output \(H(x)\) of these layers is calculated. The residual block aims to learn the residual mapping \(F(x) = H(x) - x\). The original mapping is then recast into \(F(x) + x\), making it easier to learn the identity mapping. This approach simplifies the training of deep networks significantly.

  **Why use ResNet**  
   **Deep Networks:** ResNet enables the construction of very deep networks, which is crucial for capturing intricate features in facial expressions. Deeper networks can potentially learn more complex patterns, improving the model's ability to recognize subtle emotional cues.  
  **Feature Reuse:** Residual connections allow for the direct flow of gradients during backpropagation. This promotes the reuse of features from earlier layers, enabling the network to focus on learning new, more complex features in deeper layers. In the context of facial emotion recognition, this reuse of features can enhance the model's understanding of facial expressions.  
  **Preventing Overfitting:** ResNet's ability to train deeper networks without a significant increase in the number of parameters can help prevent overfitting, especially when dealing with relatively small datasets like FER2013. Regularization techniques can also be employed alongside ResNet architectures to enhance generalization

![image](https://github.com/user-attachments/assets/35640fdc-ca67-49aa-802e-22b896cf2543)

## **Result & Discussion**

![image](https://github.com/user-attachments/assets/9afa4a99-294a-4e67-8ec0-1c541b08ee55)

![image](https://github.com/user-attachments/assets/889c78cc-e735-4eac-b055-14a0f6601157)

![image](https://github.com/user-attachments/assets/c953f6c7-24b1-48f6-a561-bec5eadf0a73)

![image](https://github.com/user-attachments/assets/f66ae1c1-04f1-4248-aa09-53cd22a7e721)

